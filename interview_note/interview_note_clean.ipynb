{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ExaOcnjFil"
      },
      "source": [
        "# Walmart Tech. P13N - ML code interview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3lfmgdobBj9f"
      },
      "outputs": [],
      "source": [
        "# Recommender System: Matrix Factorization, Neural Collaborative Filtering, Session-based recommender system\n",
        "# ** Written by Jonogseok Han (EE in P13N, email - work: jongseok.han0@walmart.com / personal: jshan.cse@gmail.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7clA8ohorzLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q_VzcEUyYn2",
        "outputId": "51947575-85de-4bfc-b142-a914af773d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNfxqaiiwG-i"
      },
      "source": [
        "## Section 0: **Data Parsing & Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k_DrH-0eBbtZ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "train_url = \"https://raw.githubusercontent.com/Jongseok-han/ml_recsys/main/interview_note/data/train.csv\"\n",
        "test_url = \"https://raw.githubusercontent.com/Jongseok-han/ml_recsys/main/interview_note/data/test.csv\"\n",
        "\n",
        "path_data = Path(\"data\")\n",
        "path_data.mkdir(exist_ok=True)\n",
        "\n",
        "path_train = path_data/\"train.csv\"\n",
        "path_test = path_data/\"test.csv\"\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "if not path_train.exists(): urlretrieve(train_url, path_train)\n",
        "if not path_test.exists(): urlretrieve(test_url, path_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q4Q0D7kC4gK"
      },
      "source": [
        "### Data Description: **[MovieLens](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriRDSbej7XE"
      },
      "source": [
        "MovieLens dataset (ml-latest-small) - we offer two files `train.csv` and `test.csv`. Each line of these files represents one rating of one movie by one user, and has the following format:\n",
        "\n",
        "    userId,movieId,rating,timestamp\n",
        "\n",
        "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars). We will normalize the ratings in (0.1 - 1.0) scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X7ceuvqh2p7H"
      },
      "outputs": [],
      "source": [
        "def load_dataset (filename):\n",
        "  input_data = []\n",
        "  with open (filename) as fin:\n",
        "    for i, line in enumerate (fin):\n",
        "      parts = line.strip().split (',')\n",
        "      input_data.append([parts[0],parts[1],float(parts[2])/5, parts[3]])\n",
        "  return np.array(input_data).astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YcuUUm1v4wZ",
        "outputId": "d9a5314b-78ed-4295-d68a-072a9d09fa33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Dataset statistics ===\n",
            "Number of Train Data: 90718 / Number of Test Data: 9715\n",
            "Min/max Ratings of Train Data: (0.1, 1.0)\n"
          ]
        }
      ],
      "source": [
        "# data statistics\n",
        "train_data = load_dataset (\"./data/train.csv\")\n",
        "test_data = load_dataset (\"./data/test.csv\")\n",
        "train_size, test_size = len(train_data), len(test_data)\n",
        "\n",
        "print (\"=== Dataset statistics ===\")\n",
        "print (\"Number of Train Data: {} / Number of Test Data: {}\".format(train_size, test_size))\n",
        "print (\"Min/max Ratings of Train Data: ({}, {})\".format(min(train_data[:,2]), max(train_data[:,2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Cqp2VQVfF6t"
      },
      "outputs": [],
      "source": [
        "# sort userID, itemID in ascending order: userID from 0 to N, itemID from 0 to M\n",
        "original_data = np.vstack((train_data, test_data))\n",
        "\n",
        "unique_users = sorted(list(set(original_data[:, 0])))\n",
        "unique_items = sorted(list(set(original_data[:, 1])))\n",
        "num_users, num_items = len(unique_users), len(unique_items)\n",
        "\n",
        "user_dic = {user:idx for (idx,user) in enumerate(unique_users)}\n",
        "item_dic = {item:idx for (idx,item) in enumerate(unique_items)}\n",
        "\n",
        "for (idx, row) in enumerate(original_data):\n",
        "    user,item = user_dic[row[0]],item_dic[row[1]]\n",
        "    original_data[idx,0],original_data[idx,1] = int(user),int(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X333AxEHr3e"
      },
      "source": [
        "## Section 1: **Matrix Factorization**\n",
        "\n",
        "The matrix factorization algorithm works by decomposing the rating matrix into the product of two lower dimensionality rectangular matrices. Letâ€™s define $K$ as the number of latent dimensions. Then, we learn a user embedding matrix $U \\in  \\mathbb{R}^{N \\times K}$ and an item embedding matrix $V \\in  \\mathbb{R}^{M \\times K}$ ($N$ and $M$ are the number of users and items, respectively).\n",
        "We will approximate a rating $R_{u,i}$ by updating $U$ and $V$ matrices.  \n",
        "$$R_{u,i} \\approx \\sum_{k=1}^{K} U_{u,k} V_{i,k}$$\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1qZyCdjsZ_8hlfIxgPbO1Ht31gqj14LVl'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd1LPFvgq1rE",
        "outputId": "7d2fbb6e-e2aa-4f5a-808c-d51f650950b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User-Item Rating Matrix - shape:(610, 9338)\n",
            "\n",
            "[[0.8 0.  0.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " ...\n",
            " [0.  0.  0.  ... 0.  0.  0. ]\n",
            " [0.9 0.  0.  ... 0.  0.  0. ]\n",
            " [0.  0.8 0.  ... 0.  0.  0. ]]\n"
          ]
        }
      ],
      "source": [
        "# build rating matrix before Matrix Factorization\n",
        "gt_matrix = np.zeros((num_users, num_items))\n",
        "\n",
        "for interaction in original_data:\n",
        "  gt_matrix[int(interaction[0]),int(interaction[1])] = float(interaction[2])\n",
        "\n",
        "print(\"User-Item Rating Matrix - shape:{}\\n\\n{}\".format(gt_matrix.shape, gt_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HAwblIMugTbt"
      },
      "outputs": [],
      "source": [
        "# define train & test set. We ignore a timestamp column since MF didn't consider sequential information\n",
        "train_data, test_data = original_data[:train_size,:3], original_data[train_size:,:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xp6jKpyfKdW5"
      },
      "outputs": [],
      "source": [
        "# class MF(nn.Module):\n",
        "#     def __init__(self, num_users, num_items, latent_dim):\n",
        "#         torch.manual_seed(0)\n",
        "#         np.random.seed(0)\n",
        "#         super(MF, self).__init__()\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################\n",
        "\n",
        "#     def forward(self, input):\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VltYcp8Ycezn"
      },
      "outputs": [],
      "source": [
        "latent_dim = 10\n",
        "mf_model = MF(num_users = num_users, num_items=num_items, latent_dim = latent_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_rxmzkdUuvvg"
      },
      "outputs": [],
      "source": [
        "# loss function & experimental setting\n",
        "# criterion = None\n",
        "optimizer = torch.optim.Adam(mf_model.parameters())\n",
        "\n",
        "batch_size = 1024\n",
        "batch_num = len(train_data)//batch_size + 1\n",
        "max_epoch = 500\n",
        "patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg4hO4Ntce2-",
        "outputId": "f2ae470a-894a-4d25-e9ee-c822ece7adf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 \ttrain_loss = 298.9193\ttest_RMSE = 1.5540\n",
            "epoch 10 \ttrain_loss = 43.3422\ttest_RMSE = 0.4986\n",
            "epoch 20 \ttrain_loss = 16.1494\ttest_RMSE = 0.3207\n",
            "epoch 30 \ttrain_loss = 8.6717\ttest_RMSE = 0.2595\n",
            "epoch 40 \ttrain_loss = 5.3630\ttest_RMSE = 0.2248\n",
            "epoch 50 \ttrain_loss = 3.6059\ttest_RMSE = 0.2015\n",
            "epoch 60 \ttrain_loss = 2.6309\ttest_RMSE = 0.1859\n",
            "epoch 70 \ttrain_loss = 2.0989\ttest_RMSE = 0.1767\n",
            "epoch 80 \ttrain_loss = 1.8099\ttest_RMSE = 0.1723\n",
            "epoch 90 \ttrain_loss = 1.6392\ttest_RMSE = 0.1706\n",
            "epoch 100 \ttrain_loss = 1.5201\ttest_RMSE = 0.1704\n",
            "\n",
            "Stop Training at epoch 107 - test_RMSE = 0.1707\n"
          ]
        }
      ],
      "source": [
        "min_RMSE, best_epoch = np.inf, 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "  # batch training\n",
        "  mf_model.train()\n",
        "  train_loss = 0\n",
        "  for batch in range(batch_num):\n",
        "    std, end = batch*batch_size, (batch+1)*batch_size\n",
        "    end = len(train_data) if (batch+1)*batch_size > len(train_data) else (batch+1)*batch_size\n",
        "    input, gt = train_data[std:end,:2], train_data[std:end,-1]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input = torch.LongTensor(input.astype(int)).to(device)\n",
        "    gt = torch.FloatTensor(gt.astype(float)).to(device)\n",
        "\n",
        "    pred = mf_model(input)\n",
        "    loss = criterion(pred,gt)\n",
        "    train_loss += loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # test evaluation\n",
        "  mf_model.eval()\n",
        "  test_input, test_gt = test_data[:,:2], test_data[:,-1]\n",
        "  test_gt = torch.FloatTensor(test_gt.astype(float))\n",
        "\n",
        "  test_pred = mf_model(torch.LongTensor(test_input.astype(int)).to(device))\n",
        "  test_RMSE = ((test_pred.detach().cpu()-test_gt)**2).sqrt().sum()/test_size\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(\"epoch {} \\ttrain_loss = {:.4f}\\ttest_RMSE = {:.4f}\".format(epoch,train_loss.item(),test_RMSE))\n",
        "\n",
        "  if test_RMSE < min_RMSE:\n",
        "    min_RMSE = test_RMSE\n",
        "    best_epoch = epoch\n",
        "  elif epoch - best_epoch >= patience:\n",
        "    print(\"\\nStop Training at epoch {} - test_RMSE = {:.4f}\".format(epoch, test_RMSE))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efcxjWNjPsXF"
      },
      "source": [
        "## Section 2: **Neural Collaborative Filtering**\n",
        "\n",
        "For NCF, we will implement a simple multi-layer perceptron (MLP) model as below.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1kEFTCa7ragS44aYmF89V958TbJpcPv-u'>\n",
        "\n",
        "The embedding layer transforms user_id and item_id to user and item latent vectors, respectively. Here, we will use U, V instead of nn.Embedding in order to focus on the performace from NCF layers.\n",
        "\n",
        "For NCF layers, please use the size of [64,512] and [512,512] and ReLU activation function. The output layer is also fully-connected (with ReLU activation) with size of [512,1] to produce a single prediction rating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN1xa-1UOTOu"
      },
      "outputs": [],
      "source": [
        "# class NCF(nn.Module):\n",
        "#     def __init__(self, num_users, num_items, latent_dim):\n",
        "#         torch.manual_seed(0)\n",
        "#         np.random.seed(0)\n",
        "#         super(NCF, self).__init__()\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################\n",
        "\n",
        "#     def forward(self, input):\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re7x4sblwCZp",
        "outputId": "88b38980-ecce-4424-a725-14665310d7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NCF(\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "ncf_model = NCF(num_users, num_items, latent_dim).to(device)\n",
        "print(ncf_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9xt3QOA42IMs"
      },
      "outputs": [],
      "source": [
        "criterion = None\n",
        "optimizer = torch.optim.Adam(ncf_model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H2wFtmgg2IBi"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "batch_num = len(train_data)//batch_size + 1\n",
        "max_epoch = 500\n",
        "patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxs13Tbf2H0W",
        "outputId": "24f2f852-c346-44b1-b9ec-3bbc49828a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 \ttrain_loss = 6.8992\ttest_RMSE = 0.1784\n",
            "epoch 10 \ttrain_loss = 3.7474\ttest_RMSE = 0.1660\n",
            "epoch 20 \ttrain_loss = 3.3262\ttest_RMSE = 0.1527\n",
            "epoch 30 \ttrain_loss = 3.0077\ttest_RMSE = 0.1458\n",
            "epoch 40 \ttrain_loss = 2.7876\ttest_RMSE = 0.1436\n",
            "epoch 50 \ttrain_loss = 2.6126\ttest_RMSE = 0.1398\n",
            "epoch 60 \ttrain_loss = 2.4437\ttest_RMSE = 0.1380\n",
            "epoch 70 \ttrain_loss = 2.3195\ttest_RMSE = 0.1372\n",
            "\n",
            "Stop Training at epoch 78 - test_RMSE = 0.1377\n"
          ]
        }
      ],
      "source": [
        "min_RMSE, best_epoch = np.inf, 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "  # batch training\n",
        "  ncf_model.train()\n",
        "  train_loss = 0\n",
        "  for batch in range(batch_num):\n",
        "    std, end = batch*batch_size, (batch+1)*batch_size\n",
        "    end = len(train_data) if (batch+1)*batch_size > len(train_data) else (batch+1)*batch_size\n",
        "    input, gt = train_data[std:end,:2], train_data[std:end,-1]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input = torch.LongTensor(input.astype(int)).to(device)\n",
        "    gt = torch.FloatTensor(gt.astype(float)).to(device)\n",
        "\n",
        "    pred = ncf_model(input)\n",
        "    loss = criterion(pred,gt)\n",
        "    train_loss += loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # test evaluation\n",
        "  ncf_model.eval()\n",
        "  test_input, test_gt = test_data[:,:2], test_data[:,-1]\n",
        "  test_gt = torch.FloatTensor(test_gt.astype(float))\n",
        "\n",
        "  test_pred = ncf_model(torch.LongTensor(test_input.astype(int)).to(device))\n",
        "  test_RMSE = ((test_pred.detach().cpu()-test_gt)**2).sqrt().sum()/test_size\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(\"epoch {} \\ttrain_loss = {:.4f}\\ttest_RMSE = {:.4f}\".format(epoch,train_loss.item(),test_RMSE))\n",
        "\n",
        "  if test_RMSE < min_RMSE:\n",
        "    min_RMSE = test_RMSE\n",
        "    best_epoch = epoch\n",
        "  elif epoch - best_epoch >= patience:\n",
        "    print(\"\\nStop Training at epoch {} - test_RMSE = {:.4f}\".format(epoch, test_RMSE))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4iLlrtdO1qP"
      },
      "source": [
        "## Section 3: **Session-based Recommender System**\n",
        "\n",
        "Session-based recommender system is a type of recommendation engine that generates personalized suggestions for users based on the actions they have taken within a session. These models use algorithms to analyze the **sequence of interactions within a session**, such as pages viewed, items added to a cart, or search queries, to predict and recommend **next item** that the user is likely to be interested in.\n",
        "\n",
        "For example, if a customer spends a session looking at various clothes, shoes and accessories, the recommender system might suggest related products like sneakers leveraging insights from the current session to enhance the shopping experience.\n",
        "<img src='https://drive.google.com/uc?export=view&id=1UH58CB1ENbngSWpGx9agqN4w2WBrFoIk'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VUIrsBQTXL",
        "outputId": "1a43d818-95a4-4a90-c6f3-ae6fabbc8963"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['0', '0', '0.8'],\n",
              "       ['0', '3536', '0.8'],\n",
              "       ['0', '6636', '0.8'],\n",
              "       ...,\n",
              "       ['568', '1590', '0.8'],\n",
              "       ['568', '1676', '0.6'],\n",
              "       ['568', '1720', '0.7']], dtype='<U32')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaVyiGZMOBOU"
      },
      "outputs": [],
      "source": [
        "# only consider positive interaction for sequential recommendation\n",
        "positive_signal = 0.5\n",
        "positive_interactions = original_data[(np.array([float(rating) for rating in original_data[:,2]]) > positive_signal)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7fBFL4-rqpH"
      },
      "outputs": [],
      "source": [
        "# re-sort userID, itemID in ascending order: userID from 0 to N, itemID from 0 to M\n",
        "unique_pos_users = sorted(list(set(positive_interactions[:, 0])))\n",
        "unique_pos_items = sorted(list(set(positive_interactions[:, 1])))\n",
        "num_items = len(unique_pos_items)\n",
        "\n",
        "pos_user_dic = {user:idx for (idx,user) in enumerate(unique_pos_users)}\n",
        "pos_item_dic = {item:idx for (idx,item) in enumerate(unique_pos_items)}\n",
        "\n",
        "for (idx, row) in enumerate(positive_interactions):\n",
        "    user, item = pos_user_dic[row[0]], pos_item_dic[row[1]]\n",
        "    positive_interactions[idx,0], positive_interactions[idx,1] = int(user),int(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAycAZpeN52U"
      },
      "outputs": [],
      "source": [
        "def train_test_split_user(data, test_ratio = 0.1):\n",
        "    (users,counts) = np.unique(data[:,0], return_counts = True)\n",
        "    users = users[counts>=10]\n",
        "\n",
        "    user_dic = {int(user):idx for (idx,user) in enumerate(users)}\n",
        "    new_data = []\n",
        "    for i in range(len(data)):\n",
        "        if int(data[i,0]) in user_dic:\n",
        "            new_data.append([int(data[i,0]),int(data[i,1]),0])\n",
        "\n",
        "    new_data = np.array(new_data)\n",
        "    sequence_dic = {int(user):[] for user in user_dic.keys()}\n",
        "\n",
        "    for i in range(len(new_data)):\n",
        "        sequence_dic[int(new_data[i,0])].append(i)\n",
        "\n",
        "    for user in sequence_dic.keys():\n",
        "        cur_test = int(test_ratio * len(sequence_dic[user]))\n",
        "        for i in range(cur_test):\n",
        "            interaction = sequence_dic[user].pop()\n",
        "            new_data[interaction,2] = 1\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6NuGBXTUIyC"
      },
      "outputs": [],
      "source": [
        "# remove user with less than 10 interactions\n",
        "# for each user, divide first 90% interactions to train & last 10% of interactions to test\n",
        "new_data = train_test_split_user(positive_interactions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gbauw4qUm_7",
        "outputId": "9bfb3c96-6d75-4c59-abc4-bcf70a1bda28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_id 0 - train ratio = 0.90, test ratio = 0.10\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "user_id = 0\n",
        "\n",
        "sample_trajectory = new_data[new_data[:,0] == user_id]\n",
        "print(\"user_id {} - train ratio = {:.2f}, test ratio = {:.2f}\".format(user_id,\\\n",
        "                                                                 (sample_trajectory[:,2] == 0).sum()/len(sample_trajectory),\\\n",
        "                                                                 (sample_trajectory[:,2] == 1).sum()/len(sample_trajectory)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUWn9ZqQ2K4U"
      },
      "outputs": [],
      "source": [
        "def sequence_generator(data, look_back = 50):\n",
        "    train, test = [],[]\n",
        "    unique_users = set(data[:,0])\n",
        "    items_per_user = {int(user):[0 for i in range(look_back)] for user in unique_users}\n",
        "\n",
        "    for (idx,row) in enumerate(data):\n",
        "      user, item, train_test = row\n",
        "      items_per_user[user] = items_per_user[user][1:]+[item+1]\n",
        "      current_items = items_per_user[user]\n",
        "      if train_test == 0:\n",
        "        train.append([current_items[:-1],current_items[-1]])\n",
        "      else:\n",
        "        test.append([current_items[:-1],current_items[-1]])\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXy3bmQX2K6v"
      },
      "outputs": [],
      "source": [
        "# convert to sequential data for sequential recommender system (look back = length of sequential time window)\n",
        "look_back = 50\n",
        "train, test = sequence_generator(new_data, look_back = look_back)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjghuHmpcA3X"
      },
      "outputs": [],
      "source": [
        "train_num, train_data, train_labels = len(train), list(), list()\n",
        "test_num, test_data, test_labels = len(test), list(), list()\n",
        "\n",
        "for i in range(train_num):\n",
        "    train_data.append(train[i][0])\n",
        "    train_labels.append(train[i][1])\n",
        "\n",
        "for i in range(test_num):\n",
        "    test_data.append(test[i][0])\n",
        "    test_labels.append(test[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYPNd64kRYmb"
      },
      "outputs": [],
      "source": [
        "# class LSTM(nn.Module):\n",
        "    # def __init__(self, num_items, emb_size, hidden_dim, n_layers=1):\n",
        "#         torch.manual_seed(0)\n",
        "#         np.random.seed(0)\n",
        "#         super(LSTM, self).__init__()\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################\n",
        "\n",
        "#     def forward(self, input):\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS8UszdkNyNm"
      },
      "outputs": [],
      "source": [
        "item_emb_dim = 32\n",
        "hidden_dim = 32\n",
        "\n",
        "lstm_model = LSTM(num_items+1, item_emb_dim, hidden_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXwA59L-bEMB"
      },
      "outputs": [],
      "source": [
        "# loss function & experimental setting\n",
        "criterion = None\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters())\n",
        "\n",
        "batch_size = 1024\n",
        "batch_num = len(train_data)//batch_size + 1\n",
        "max_epoch = 500\n",
        "patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq1oqKCzSnF_"
      },
      "outputs": [],
      "source": [
        "def mean_reciprocal_rank(pred, gt):\n",
        "  probs = nn.functional.softmax(test_pred, dim=1)\n",
        "\n",
        "  current_val = np.zeros((test_num,1))\n",
        "  for i in range(test_num):\n",
        "      current_test_label = test_labels[i]\n",
        "      current_val[i] = probs[i,test_labels[i]]\n",
        "\n",
        "  ranks = np.count_nonzero((probs - current_val)>0,axis=1)\n",
        "  MRR = 0\n",
        "  for i in range(test_num):\n",
        "    MRR += 1/(ranks[i]+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ucm-1aEF2LBl"
      },
      "outputs": [],
      "source": [
        "max_mrr, best_epoch = 0, 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "  # batch training\n",
        "  train_loss = 0\n",
        "  lstm_model.train()\n",
        "  for batch in range(batch_num):\n",
        "    std, end = batch*batch_size, (batch+1)*batch_size\n",
        "    end = train_num if (batch+1)*batch_size > train_num else (batch+1)*batch_size\n",
        "    input, gt = train_data[std:end], train_labels[std:end]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input = torch.LongTensor(input).to(device)\n",
        "    gt = torch.LongTensor(gt).to(device)\n",
        "\n",
        "    pred = lstm_model(input)\n",
        "    loss = criterion(pred,gt)\n",
        "    train_loss += loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # test evaluation\n",
        "  lstm_model.eval()\n",
        "  test_input = torch.LongTensor(test_data).to(device)\n",
        "  test_pred = lstm_model(test_input)\n",
        "\n",
        "  test_gt = torch.LongTensor(test_labels)\n",
        "  MRR = mean_reciprocal_rank(test_pred.detach().cpu().numpy(), test_gt)\n",
        "\n",
        "  MRR /= test_num\n",
        "  if epoch % 10==0:\n",
        "    print(\"epoch {} \\ttrain_loss = {:.4f}\\ttest_MRR = {:.4f}\".format(epoch,train_loss,MRR))\n",
        "\n",
        "  if max_mrr < MRR:\n",
        "    max_mrr = MRR\n",
        "    best_epoch = epoch\n",
        "  elif epoch - best_epoch >= patience:\n",
        "    print(\"Stop Training at epoch {} - test_MRR = {:.4f}\".format(epoch, MRR))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJuAbe3r4Xjp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:,:x.size(1),:]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw_xwk5ST7ek"
      },
      "outputs": [],
      "source": [
        "# class Transformer(nn.Module):\n",
        "#     def __init__(self, num_items, item_emb_dim, hidden_dim, n_head=4, n_layers=1):\n",
        "#         torch.manual_seed(0)\n",
        "#         np.random.seed(0)\n",
        "#         super(Transformer, self).__init__()\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################\n",
        "\n",
        "#     def forward(self, input):\n",
        "\n",
        "#         #### YOUR CODE GOES HERE ####\n",
        "\n",
        "#         #############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KuPSSjuThOp"
      },
      "outputs": [],
      "source": [
        "item_emb_dim = 32\n",
        "hidden_dim = 32\n",
        "\n",
        "transformer_model = Transformer(num_items+1, item_emb_dim, hidden_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3nPefeI7Mpv"
      },
      "outputs": [],
      "source": [
        "# loss function & experimental setting\n",
        "criterion = None\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters())\n",
        "\n",
        "batch_size = 1024\n",
        "batch_num = len(train_data)//batch_size + 1\n",
        "max_epoch = 500\n",
        "patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPt3XVV13vQm",
        "outputId": "1abd0e46-27f3-47ce-ec95-bfa82be0d0e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 \ttrain_loss = 621.2628\ttest_MRR = 0.0108\n"
          ]
        }
      ],
      "source": [
        "max_mrr, best_epoch = 0, 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "  # batch training\n",
        "  train_loss = 0\n",
        "  transformer_model.train()\n",
        "  for batch in range(batch_num):\n",
        "    std, end = batch*batch_size, (batch+1)*batch_size\n",
        "    end = train_num if (batch+1)*batch_size > train_num else (batch+1)*batch_size\n",
        "    input, gt = train_data[std:end], train_labels[std:end]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    input = torch.LongTensor(input).to(device)\n",
        "    gt = torch.LongTensor(gt).to(device)\n",
        "\n",
        "    pred = transformer_model(input)\n",
        "    loss = criterion(pred,gt)\n",
        "    train_loss += loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # test evaluation\n",
        "  transformer_model.eval()\n",
        "  test_input = torch.LongTensor(test_data).to(device)\n",
        "  test_pred = transformer_model(test_input)\n",
        "\n",
        "  test_gt = torch.LongTensor(test_labels)\n",
        "  MRR = mean_reciprocal_rank(test_pred.detach().cpu().numpy(), test_gt)\n",
        "\n",
        "  MRR /= test_num\n",
        "  if epoch % 10==0:\n",
        "    print(\"epoch {} \\ttrain_loss = {:.4f}\\ttest_MRR = {:.4f}\".format(epoch,train_loss,MRR))\n",
        "\n",
        "  if max_mrr < MRR:\n",
        "    max_mrr = MRR\n",
        "    best_epoch = epoch\n",
        "  elif epoch - best_epoch >= patience:\n",
        "    print(\"Stop Training at epoch {} - test_MRR = {:.4f}\".format(epoch, MRR))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpTtxLee3vUP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y33BuxxJ1DO6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
